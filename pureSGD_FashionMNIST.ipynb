{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Yr64xAkSbn8CVFqSu62-cRi8fR9c2YKt","timestamp":1727100309947}],"gpuType":"T4","authorship_tag":"ABX9TyOzEU0ZZOcAks4VNnp/RdH+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"gp9mNWPSHL0T","executionInfo":{"status":"ok","timestamp":1727148315425,"user_tz":-420,"elapsed":19026,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","import torchvision\n","from torchvision import datasets, transforms\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_digits\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import time\n","from tensorflow import keras\n","import psutil\n","device='cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n","])"],"metadata":{"id":"qftJ0nILHFhY","executionInfo":{"status":"ok","timestamp":1727148318570,"user_tz":-420,"elapsed":594,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"],"metadata":{"id":"Cq0rCrWm6zbt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727148327291,"user_tz":-420,"elapsed":6323,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}},"outputId":"a3162ec1-3883-4821-dfe6-5c25cc1bd9a2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"]}]},{"cell_type":"code","source":["# Flatten the images\n","x_train = x_train.reshape(-1, 28 * 28)\n","x_test = x_test.reshape(-1, 28 * 28)\n","\n","# Convert to tensors\n","x_train_tensor = torch.from_numpy(x_train).float().to(device)\n","y_train_tensor = torch.from_numpy(y_train).long().to(device)\n","x_test_tensor = torch.from_numpy(x_test).float().to(device)\n","y_test_tensor = torch.from_numpy(y_test).long().to(device)\n","\n","# Create the training dataset\n","train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n","\n","# Define the size of the validation set (e.g., 20% of the training set)\n","val_size = int(0.2 * len(train_dataset))\n","train_size = len(train_dataset) - val_size\n","\n","# Split the dataset into training and validation datasets\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","\n","# Create dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)  # No shuffling for validation\n","test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ueiqAL0t-cXV","executionInfo":{"status":"ok","timestamp":1727148327851,"user_tz":-420,"elapsed":565,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}},"outputId":"eaa5d410-59ba-407d-849a-65f6b6823c04"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-8f83f1f3566b>:6: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n","  x_train_tensor = torch.from_numpy(x_train).float().to(device)\n"]}]},{"cell_type":"code","source":["class MLP_FashionMnist(nn.Module):\n","    def __init__(self):\n","        super(MLP_FashionMnist, self).__init__()\n","        self.fc1 = nn.Linear(784, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, 10)\n","        self.relu1 = nn.ReLU()\n","        self.relu2 = nn.ReLU()\n","\n","    def forward(self, x):\n","        out = self.relu1(self.fc1(x))\n","        out = self.relu2(self.fc2(out))\n","        out = self.fc3(out)\n","        return out"],"metadata":{"id":"29nbpzTs7EV9","executionInfo":{"status":"ok","timestamp":1727148335396,"user_tz":-420,"elapsed":710,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["model = MLP_FashionMnist().to(device)"],"metadata":{"id":"UBNz_9Of-kUS","executionInfo":{"status":"ok","timestamp":1727148614204,"user_tz":-420,"elapsed":705,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["#loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"],"metadata":{"id":"iZaWxk5qBNNz","executionInfo":{"status":"ok","timestamp":1727148614817,"user_tz":-420,"elapsed":5,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["class EarlyStopping:\n","    def __init__(self, patience=5, delta=0):\n","        self.patience = patience\n","        self.delta = delta\n","        self.best_loss = None\n","        self.counter = 0\n","\n","    def __call__(self, val_loss):\n","        if self.best_loss is None:\n","            self.best_loss = val_loss\n","        elif val_loss < self.best_loss - self.delta:\n","            self.best_loss = val_loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                return True  # Indicate convergence\n","        return False  # Continue training"],"metadata":{"id":"W2FrVhxdD6cS","executionInfo":{"status":"ok","timestamp":1727148340808,"user_tz":-420,"elapsed":813,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["early_stopping = EarlyStopping(patience=5, delta=0.001)"],"metadata":{"id":"2JksctnaD_H8","executionInfo":{"status":"ok","timestamp":1727148616657,"user_tz":-420,"elapsed":4,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def calculate_validation_loss(model, data_loader, criterion):\n","    model.eval()  # Set the model to evaluation mode\n","    total_loss = 0.0\n","    total_samples = 0\n","\n","    with torch.no_grad():  # Disable gradient calculation for efficiency\n","        for data, target in data_loader:\n","            data = data.view(data.size(0), -1).to(device)  # Flatten the images\n","            target = target.to(device)\n","\n","            # Forward pass\n","            outputs = model(data)\n","\n","            # Calculate loss\n","            loss = criterion(outputs, target)\n","\n","            # Accumulate loss\n","            total_loss += loss.item() * data.size(0)  # Multiply by batch size to get total loss\n","            total_samples += data.size(0)  # Count total samples\n","\n","    average_loss = total_loss / total_samples  # Average loss over all samples\n","    return average_loss"],"metadata":{"id":"Ls0gOTUbE7vS","executionInfo":{"status":"ok","timestamp":1727148342479,"user_tz":-420,"elapsed":6,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def train(model, train_loader, criterion, optimizer, num_epochs):\n","    t1 = time.time()\n","    model.train()  # Set the model to training mode\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            data, target = data.view(data.size(0), -1).to(device), target.to(device)  # Flatten the images\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","            # Forward pass\n","            outputs = model(data)\n","            # Calculate loss\n","            loss = criterion(outputs, target)\n","            # Backward pass\n","            loss.backward()\n","            # Optimize weights\n","            optimizer.step()\n","            # Accumulate loss\n","            running_loss += loss.item()\n","        val_loss = calculate_validation_loss(model, val_loader, criterion)\n","        if early_stopping(val_loss):\n","            print(\"Early stopping triggered.\")\n","            break\n","        # Print loss for the epoch\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n","    t2 = time.time()\n","    print(f\"total training time : {t2-t1}\")"],"metadata":{"id":"dNAY3m-iBLD9","executionInfo":{"status":"ok","timestamp":1727148344382,"user_tz":-420,"elapsed":5,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["train(model, train_loader, criterion, optimizer, 100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O9Azx-_hB6l9","executionInfo":{"status":"ok","timestamp":1727148657669,"user_tz":-420,"elapsed":39058,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}},"outputId":"756712bc-07f1-46a4-d2e2-e7f8e58c1dac"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Loss: 0.7823\n","Epoch [2/100], Loss: 0.4443\n","Epoch [3/100], Loss: 0.3933\n","Epoch [4/100], Loss: 0.3652\n","Epoch [5/100], Loss: 0.3445\n","Epoch [6/100], Loss: 0.3261\n","Epoch [7/100], Loss: 0.3129\n","Epoch [8/100], Loss: 0.3020\n","Epoch [9/100], Loss: 0.2916\n","Epoch [10/100], Loss: 0.2820\n","Epoch [11/100], Loss: 0.2727\n","Epoch [12/100], Loss: 0.2639\n","Epoch [13/100], Loss: 0.2569\n","Epoch [14/100], Loss: 0.2504\n","Epoch [15/100], Loss: 0.2425\n","Epoch [16/100], Loss: 0.2376\n","Epoch [17/100], Loss: 0.2314\n","Epoch [18/100], Loss: 0.2282\n","Epoch [19/100], Loss: 0.2212\n","Epoch [20/100], Loss: 0.2168\n","Epoch [21/100], Loss: 0.2108\n","Epoch [22/100], Loss: 0.2055\n","Epoch [23/100], Loss: 0.1995\n","Early stopping triggered.\n","total training time : 38.14391303062439\n"]}]},{"cell_type":"code","source":["def calculate_accuracy(model, data_loader):\n","    model.eval()  # Set the model to evaluation mode\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():  # Disable gradient calculation for efficiency\n","        for data, target in data_loader:\n","            data = data.view(data.size(0), -1).to(device)  # Flatten the images\n","            target = target.to(device)\n","\n","            # Forward pass\n","            outputs = model(data)\n","            _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest probability\n","\n","            # Update correct and total counts\n","            total += target.size(0)\n","            correct += (predicted == target).sum().item()\n","\n","    accuracy = 100 * correct / total  # Calculate accuracy as a percentage\n","    return accuracy"],"metadata":{"id":"-5l0qQnECnrK","executionInfo":{"status":"ok","timestamp":1727148395298,"user_tz":-420,"elapsed":591,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["calculate_accuracy(model, test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7yGJMWxOCqWJ","executionInfo":{"status":"ok","timestamp":1727148676465,"user_tz":-420,"elapsed":612,"user":{"displayName":"Fhilip Yanus","userId":"02752878488140676545"}},"outputId":"0d2b9962-be97-4a58-8299-3035417bcc45"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["88.13"]},"metadata":{},"execution_count":29}]}]}